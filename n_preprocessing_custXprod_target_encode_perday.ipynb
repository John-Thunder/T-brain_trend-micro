{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(81977, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train_set = pd.read_csv(\"dataset/training-set.csv\")\n",
    "df_test_set = pd.read_csv(\"dataset/testing-set.csv\")\n",
    "\n",
    "df_train_set[\"type\"] = 'train'\n",
    "df_test_set[\"type\"] = 'test'\n",
    "\n",
    "df_set = pd.concat([df_train_set, df_test_set], axis = 0)\n",
    "print (df_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv(\"dataset/file_cust_prod_ct_uct.csv\")\n",
    "df_ = pd.merge(df_, df_set, how='left', on='file_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45694, 41) (6824, 41) (29376, 41)\n"
     ]
    }
   ],
   "source": [
    "df_train_set = df_[(df_.type == 'train')&(df_.date_min <= 51)]\n",
    "df_val_set = df_[(df_.type == 'train')&(df_.date_min > 51)]\n",
    "df_test_set = df_[(df_.type == 'test')]\n",
    "\n",
    "print (df_train_set.shape, df_val_set.shape, df_test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for month_ in [\"train1\",\"train2\",\"train3\",\"valid\"]:\n",
    "    df_ = pd.read_pickle(\"dataset/\" + str(month_) + \".pkl\")\n",
    "    df = pd.concat([df, df_], axis=0)\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54250245, 7) (48899216, 7) (5351029, 7)\n"
     ]
    }
   ],
   "source": [
    "df_train = df[df.file_id.isin(df_train_set.file_id)]\n",
    "df_val = df[df.file_id.isin(df_val_set.file_id)]\n",
    "\n",
    "print (df.shape, df_train.shape, df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for month_ in [\"test_1\",\"test_2\",\"test_3\"]:\n",
    "    df_ = pd.read_pickle(\"dataset/\" + str(month_) + \".pkl\")\n",
    "    df = pd.concat([df, df_], axis=0)\n",
    "\n",
    "df_test = pd.concat([df, df_val], axis=0)\n",
    "df_test = df_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['custXprod_id'] = df_train.customer_id + df_train.product_id\n",
    "df_test['custXprod_id'] = df_test.customer_id + df_test.product_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 篩選不重要顧客"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ = pd.read_csv(\"dataset/customer_id.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ap = g_[(g_.cust_count > 1) & (g_.cust_date_min < 61) & (g_.cust_date_max > 60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33706609, 8) (32989698, 8)\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train[df_train['customer_id'].isin(g_ap.customer_id)].reset_index(drop=True)\n",
    "df_test = df_test[df_test['customer_id'].isin(g_ap.customer_id)].reset_index(drop=True)\n",
    "\n",
    "print (df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_set, how=\"left\", on=\"file_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    assert len(trn_series) == len(target)\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by = trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "\n",
    "    return add_noise(ft_trn_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "# 第三天顧客的分數 = 第一天 + 第二天的 target encoding\n",
    "\n",
    "old_len = 0\n",
    "trn = df_train[(df_train.date_new == 1)]\n",
    "trn[\"cust_enc\"] = None\n",
    "for day_ in range(2, np.max(df_train.date_new)+1):\n",
    "    df_before = df_train[(df_train.date_new < day_)] # 這天以前\n",
    "    trn_ = target_encode(\n",
    "        trn_series = df_before[\"custXprod_id\"], target = df_before.label, \n",
    "        min_samples_leaf=100, smoothing=10, noise_level=0.01)\n",
    "    df_before[\"cust_enc\"] = trn_\n",
    "\n",
    "    f = {'cust_enc': ['mean']} # 算出這天之前的分數\n",
    "    g_cust = df_before.groupby([\"custXprod_id\"]).aggregate(f).reset_index()\n",
    "    g_cust.columns = [\"custXprod_id\", \"cust_enc\"]\n",
    "\n",
    "    df_day = df_train[(df_train.date_new == day_)] # 連接當天資料\n",
    "    df_day = pd.merge(df_day, g_cust, how=\"left\", on=\"custXprod_id\")\n",
    "    trn = pd.concat([trn, df_day], axis=0)\n",
    "\n",
    "trn.cust_enc = pd.to_numeric(trn.cust_enc, errors = 'coerce')\n",
    "print (trn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = {'cust_enc': ['mean', 'std', 'max', 'min']}\n",
    "g_train = trn.groupby([\"file_id\"]).aggregate(f).reset_index()\n",
    "g_train.columns = [\"file_id\", \"custXprod_enc_mean\", \"custXprod_enc_std\",\n",
    "                   \"custXprod_enc_max\", \"custXprod_enc_min\"]\n",
    "\n",
    "print (g_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(series, noise_level):\n",
    "    return series * (1 + noise_level * np.random.randn(len(series)))\n",
    "\n",
    "def target_encode(trn_series=None, \n",
    "                  tst_series=None, \n",
    "                  target=None, \n",
    "                  min_samples_leaf=1, \n",
    "                  smoothing=1,\n",
    "                  noise_level=0):\n",
    "    \"\"\"\n",
    "    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n",
    "    https://kaggle2.blob.core.windows.net/forum-message-attachments/225952/7441/high%20cardinality%20categoricals.pdf\n",
    "    trn_series : training categorical feature as a pd.Series\n",
    "    tst_series : test categorical feature as a pd.Series\n",
    "    target : target data as a pd.Series\n",
    "    min_samples_leaf (int) : minimum samples to take category average into account\n",
    "    smoothing (int) : smoothing effect to balance categorical average vs prior  \n",
    "    \"\"\" \n",
    "    assert len(trn_series) == len(target)\n",
    "    assert trn_series.name == tst_series.name\n",
    "    temp = pd.concat([trn_series, target], axis=1)\n",
    "    # Compute target mean \n",
    "    averages = temp.groupby(by = trn_series.name)[target.name].agg([\"mean\", \"count\"])\n",
    "    # Compute smoothing\n",
    "    smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n",
    "    # Apply average function to all target data\n",
    "    prior = target.mean()\n",
    "    # The bigger the count the less full_avg is taken into account\n",
    "    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n",
    "    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n",
    "    # Apply averages to trn and tst series\n",
    "    ft_trn_series = pd.merge(\n",
    "        trn_series.to_frame(trn_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=trn_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_trn_series.index = trn_series.index \n",
    "    ft_tst_series = pd.merge(\n",
    "        tst_series.to_frame(tst_series.name),\n",
    "        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n",
    "        on=tst_series.name,\n",
    "        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n",
    "    # pd.merge does not keep the index so restore it\n",
    "    ft_tst_series.index = tst_series.index\n",
    "\n",
    "    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, sub = target_encode(df_train[\"custXprod_id\"], \n",
    "                         df_test[\"custXprod_id\"], \n",
    "                         target=df_train.label, \n",
    "                         min_samples_leaf=100,\n",
    "                         smoothing=10,\n",
    "                         noise_level=0.01)\n",
    "df_test[\"cust_enc\"] = sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = {'cust_enc': ['mean', 'std', 'max', 'min']}\n",
    "\n",
    "g_test = df_test.groupby([\"file_id\"]).aggregate(f).reset_index()\n",
    "g_test.columns = [\"file_id\", \"custXprod_enc_mean\", \"custXprod_enc_std\",\n",
    "                  \"custXprod_enc_max\", \"custXprod_enc_min\"]\n",
    "g = pd.concat([g_train, g_test], axis=0)\n",
    "\n",
    "f = {'custXprod_enc_mean': 'mean','custXprod_enc_std': 'mean',\n",
    "     'custXprod_enc_max': 'max','custXprod_enc_min': 'min'}\n",
    "g = g.groupby([\"file_id\"]).aggregate(f).reset_index()\n",
    "g.columns = [\"file_id\", \"custXprod_enc_mean\", \"custXprod_enc_std\",\n",
    "             \"custXprod_enc_max\", \"custXprod_enc_min\"]\n",
    "print (g_train.shape, g_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g['custXprod_enc_mean'] = g['custXprod_enc_mean'].fillna(g['custXprod_enc_mean'].mean())\n",
    "g['custXprod_enc_min'] = g['custXprod_enc_min'].fillna(g['custXprod_enc_min'].min())\n",
    "g['custXprod_enc_max'] = g['custXprod_enc_max'].fillna(g['custXprod_enc_max'].max())\n",
    "g['custXprod_enc_std']=g['custXprod_enc_std'].fillna(-999.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.to_csv(\"dataset/custXprod_enc_all_perday.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
